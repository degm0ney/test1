"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–ª–ª–µ–∫—Ü–∏–π –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ñ–∞–π–ª–æ–≤ –∫–æ–ª–ª–µ–∫—Ü–∏–π
"""

import asyncio
import aiofiles
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import time
from datetime import datetime

from config import config
from logger import logger
from async_downloader import AsyncDownloader
from gift_parser import GiftParser
from data_manager import DataManager
from cache_manager import CacheManager

class CollectionManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–π –ø–æ–¥–∞—Ä–∫–æ–≤"""
    
    def __init__(self):
        self.downloader = AsyncDownloader()
        self.gift_parser = GiftParser()
        self.data_manager = DataManager()
        self.cache_manager = CacheManager()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.processing_stats = {
            'total_collections': 0,
            'completed_collections': 0,
            'total_urls': 0,
            'processed_urls': 0,
            'successful_parses': 0,
            'failed_parses': 0,
            'skipped_urls': 0,
            'start_time': None,
            'end_time': None
        }
    
    async def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞
            await self.cache_manager.load_cache()
            
            # –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            await self.data_manager.start_auto_save()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
            async with self.downloader:
                health_check = await self.downloader.health_check()
                if not health_check:
                    logger.log_warning("Downloader health check failed, but continuing...")
            
            logger.log_info("Collection manager initialized successfully")
            return True
            
        except Exception as e:
            logger.log_error(f"Failed to initialize collection manager: {e}")
            return False
    
    async def load_collection_urls(self, collection_name: str) -> List[str]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç URL'—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞
        
        Args:
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ URL'–æ–≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        """
        collection_file = config.MATERIALS_DIR / f"{collection_name}.txt"
        
        if not collection_file.exists():
            logger.log_error(f"Collection file not found: {collection_file}")
            return []
        
        urls = []
        try:
            async with aiofiles.open(collection_file, 'r', encoding='utf-8') as f:
                async for line in f:
                    url = line.strip()
                    if url and url.startswith('https://t.me/nft/'):
                        urls.append(url)
            
            logger.log_info(f"Loaded {len(urls)} URLs for collection {collection_name}")
            return urls
            
        except Exception as e:
            logger.log_error(f"Failed to load URLs for collection {collection_name}: {e}")
            return []
    
    async def process_collection(self, collection_name: str, 
                               resume_mode: bool = True) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∫–æ–ª–ª–µ–∫—Ü–∏—é
        
        Args:
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            resume_mode: –ï—Å–ª–∏ True, –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ URL
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        """
        start_time = time.time()
        
        logger.log_collection_start(collection_name, 0)  # –û–±–Ω–æ–≤–∏–º –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ URL
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º URL'—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        all_urls = await self.load_collection_urls(collection_name)
        if not all_urls:
            return self._create_collection_stats(collection_name, 0, 0, 0, 0)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ URL –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º resume
        if resume_mode:
            urls_to_process = self.cache_manager.filter_unprocessed_urls(all_urls)
            skipped_count = len(all_urls) - len(urls_to_process)
            logger.log_info(f"Resume mode: processing {len(urls_to_process)} URLs, "
                           f"skipping {skipped_count} already processed")
        else:
            urls_to_process = all_urls
            skipped_count = 0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º URL
        logger.log_collection_start(collection_name, len(urls_to_process))
        
        if not urls_to_process:
            logger.log_info(f"Collection {collection_name} already fully processed")
            return self._create_collection_stats(collection_name, len(all_urls), 0, 0, skipped_count)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL'—ã –±–∞—Ç—á–∞–º–∏
        processed_count = 0
        success_count = 0
        error_count = 0
        
        async with self.downloader:
            # –°–æ–∑–¥–∞–µ–º progress callback
            def create_progress_callback():
                last_update = {'time': time.time(), 'count': 0}
                
                async def progress_callback(current: int, total: int):
                    nonlocal last_update
                    now = time.time()
                    if now - last_update['time'] >= 5.0:  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
                        speed = (current - last_update['count']) / (now - last_update['time'])
                        logger.log_info(f"Collection {collection_name}: {current}/{total} "
                                       f"({(current/total)*100:.1f}%) - {speed:.1f} URLs/sec")
                        last_update['time'] = now
                        last_update['count'] = current
                
                return progress_callback
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL'—ã —Å –ø–æ–º–æ—â—å—é downloader
            results = await self.downloader.download_with_batching(
                urls_to_process, 
                config.BATCH_SIZE,
                create_progress_callback()
            )
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for url, html_content, error in results:
                processed_count += 1
                
                if html_content is not None:
                    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–∞—Ä–∫–∞
                    gift_data = self.gift_parser.parse_gift_data(html_content, url)
                    
                    if gift_data and self.gift_parser.validate_gift_data(gift_data):
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–∞—Ä–æ–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
                        await self.data_manager.add_gifts_to_collection(collection_name, [gift_data])
                        success_count += 1
                        
                        # –û—Ç–º–µ—á–∞–µ–º URL –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
                        self.cache_manager.mark_url_processed(
                            url, 'active', 
                            {'gift_id': gift_data.get('gift_id'), 'collection': collection_name}
                        )
                    else:
                        error_count += 1
                        self.cache_manager.mark_url_processed(url, 'parse_failed')
                else:
                    error_count += 1
                    status = 'deleted' if error == "404 Not Found" else 'error'
                    self.cache_manager.mark_url_processed(url, status, {'error': error})
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if processed_count % config.SAVE_INTERVAL == 0:
                    await self._periodic_save(collection_name, processed_count, success_count, error_count)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        await self.data_manager.save_collection(collection_name)
        await self.cache_manager.save_cache()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        await self.cache_manager.update_collection_progress(
            collection_name, len(all_urls), processed_count, success_count, error_count
        )
        
        duration = time.time() - start_time
        logger.log_collection_complete(collection_name, success_count, error_count, duration)
        
        return self._create_collection_stats(
            collection_name, len(all_urls), processed_count, success_count, error_count, duration
        )
    
    async def _periodic_save(self, collection_name: str, processed: int, success: int, errors: int):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∫—ç—à–∞"""
        logger.log_info(f"Periodic save - {collection_name}: processed {processed}, "
                       f"success {success}, errors {errors}")
        
        await self.data_manager.save_collection(collection_name)
        await self.cache_manager.save_cache()
    
    def _create_collection_stats(self, collection_name: str, total_urls: int, 
                               processed: int, success: int, errors: int, 
                               duration: float = 0) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        return {
            'collection_name': collection_name,
            'total_urls': total_urls,
            'processed_urls': processed,
            'successful_parses': success,
            'failed_parses': errors,
            'success_rate': (success / processed) * 100 if processed > 0 else 0,
            'duration': duration,
            'urls_per_second': processed / duration if duration > 0 else 0
        }
    
    async def process_all_collections(self, resume_mode: bool = True) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏–∑ file_list.txt
        
        Args:
            resume_mode: –ï—Å–ª–∏ True, –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ URL
            
        Returns:
            –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        self.processing_stats['start_time'] = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–ª–µ–∫—Ü–∏–π
        collections = await self._load_collections_list()
        if not collections:
            logger.log_error("No collections found to process")
            return self.processing_stats
        
        self.processing_stats['total_collections'] = len(collections)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL
        total_urls = 0
        for collection_name in collections:
            urls = await self.load_collection_urls(collection_name)
            total_urls += len(urls)
        
        self.processing_stats['total_urls'] = total_urls
        logger.log_startup(len(collections), total_urls)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
        collection_results = {}
        
        for i, collection_name in enumerate(collections, 1):
            logger.log_info(f"Processing collection {i}/{len(collections)}: {collection_name}")
            
            try:
                collection_stats = await self.process_collection(collection_name, resume_mode)
                collection_results[collection_name] = collection_stats
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self.processing_stats['completed_collections'] += 1
                self.processing_stats['processed_urls'] += collection_stats['processed_urls']
                self.processing_stats['successful_parses'] += collection_stats['successful_parses']
                self.processing_stats['failed_parses'] += collection_stats['failed_parses']
                
            except Exception as e:
                logger.log_error(f"Failed to process collection {collection_name}: {e}")
                collection_results[collection_name] = {
                    'error': str(e),
                    'processed_urls': 0,
                    'successful_parses': 0,
                    'failed_parses': 0
                }
        
        self.processing_stats['end_time'] = time.time()
        self.processing_stats['total_duration'] = (
            self.processing_stats['end_time'] - self.processing_stats['start_time']
        )
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        await self.data_manager.save_all_collections()
        await self.cache_manager.save_cache(force=True)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        final_stats = {
            **self.processing_stats,
            'collection_results': collection_results,
            'downloader_stats': self.downloader.get_stats(),
            'cache_stats': self.cache_manager.get_cache_stats()
        }
        
        logger.display_final_stats(final_stats)
        
        return final_stats
    
    async def _load_collections_list(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–ª–µ–∫—Ü–∏–π –∏–∑ file_list.txt"""
        if not config.FILE_LIST.exists():
            logger.log_error(f"File list not found: {config.FILE_LIST}")
            return []
        
        collections = []
        try:
            async with aiofiles.open(config.FILE_LIST, 'r', encoding='utf-8') as f:
                async for line in f:
                    collection_name = line.strip()
                    if collection_name and not collection_name.startswith('#'):
                        # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .txt –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
                        if collection_name.endswith('.txt'):
                            collection_name = collection_name[:-4]
                        collections.append(collection_name)
            
            logger.log_info(f"Loaded {len(collections)} collections from file list")
            return collections
            
        except Exception as e:
            logger.log_error(f"Failed to load collections list: {e}")
            return []
    
    async def get_processing_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        overall_progress = self.cache_manager.get_overall_progress()
        downloader_stats = self.downloader.get_stats()
        
        return {
            'processing_stats': self.processing_stats,
            'overall_progress': overall_progress,
            'downloader_stats': downloader_stats,
            'cache_stats': self.cache_manager.get_cache_stats()
        }
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        await self.data_manager.stop_auto_save()
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π
        save_results = await self.data_manager.save_all_collections()
        saved_count = sum(1 for success in save_results.values() if success)
        logger.log_info(f"üíæ –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {saved_count} –∫–æ–ª–ª–µ–∫—Ü–∏–π")
        
        if hasattr(self.downloader, 'close_session'):
            await self.downloader.close_session()
        
        await self.cache_manager.save_cache(force=True)
        logger.log_info("üßπ –û—á–∏—Å—Ç–∫–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")